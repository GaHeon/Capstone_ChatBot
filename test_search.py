from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch
import multiprocessing

def test_search(query, k=5):
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    print("-" * 50)
    
    # CPU ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
    num_threads = max(1, multiprocessing.cpu_count() // 2)
    torch.set_num_threads(num_threads)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cpu"
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': device}
    )
    
    # FAISS DB ë¡œë“œ
    db = FAISS.load_local("embed_faiss", embeddings, allow_dangerous_deserialization=True)
    
    # ê²€ìƒ‰ ì‹¤í–‰
    docs = db.similarity_search_with_score(query, k=k)
    
    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
    def distance_to_similarity(distance):
        return 1 / (1 + distance)
    
    # ê²°ê³¼ ì¶œë ¥
    for i, (doc, score) in enumerate(docs, 1):
        similarity = distance_to_similarity(score)
        print(f"\nğŸ“„ ê²°ê³¼ {i} (ìœ ì‚¬ë„: {similarity:.3f})")
        print(f"ì¶œì²˜: {doc.metadata['source']} / ì²­í¬ ë²ˆí˜¸: {doc.metadata['chunk']}")
        print(f"ë‚´ìš©: {doc.page_content[:200]}...")
        print("-" * 50)

if __name__ == "__main__":
    while True:
        query = input("\nê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥): ")
        if query.lower() == 'q':
            break
        test_search(query) 