import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os
import torch
import gc
import re
import traceback

# âœ… Streamlit ì„¤ì •
st.set_page_config(page_title="LoRA QA Chatbot", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ§  LoRA ê¸°ë°˜ Radiation QA Chatbot")

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "model" not in st.session_state:
    st.session_state.model = None
if "tokenizer" not in st.session_state:
    st.session_state.tokenizer = None
if "db" not in st.session_state:
    st.session_state.db = None
if "device" not in st.session_state:
    st.session_state.device = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸ“¦ ë²¡í„° DB ë¡œë”©
try:
    if st.session_state.db is None:
        with st.spinner("ğŸ”„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”© ì¤‘..."):
            if not os.path.exists("embed_faiss"):
                st.error("âŒ ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € create_embeddings.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                st.stop()
                
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
                model_kwargs={'device': st.session_state.device}
            )
            db = FAISS.load_local("embed_faiss", embeddings, allow_dangerous_deserialization=True)
            st.session_state.db = db
except Exception as e:
    st.error(f"âŒ ë²¡í„° DB ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    st.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
    st.error(traceback.format_exc())
    st.stop()

# âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if user_query:
    try:
        # ğŸ” ë¬¸ì„œ ê²€ìƒ‰
        docs = st.session_state.db.similarity_search_with_score(user_query, k=5)

        # ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜ í•¨ìˆ˜
        def distance_to_similarity(distance):
            return 1 / (1 + distance)

        # ì •ë ¬ ë° í•„í„°ë§ (ìœ ì‚¬ë„ 0.3 ì´ìƒë§Œ ì‚¬ìš©)
        filtered_docs = []
        for doc, score in docs:
            sim = distance_to_similarity(score)
            if sim > 0.3:
                filtered_docs.append((doc, sim))

        if not filtered_docs:
            filtered_docs = [(doc, distance_to_similarity(score)) for doc, score in docs[:3]]

        # ìœ ì‚¬ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        filtered_docs.sort(key=lambda x: x[1], reverse=True)

        # ğŸ‘€ ê²€ìƒ‰ëœ ì²­í¬ ì‹œê°í™”
        with st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ ë³´ê¸°", expanded=True):
            st.markdown("### ğŸ“š ê²€ìƒ‰ ê²°ê³¼")
            for i, (doc, sim) in enumerate(filtered_docs, 1):
                source = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
                chunk_num = doc.metadata.get("chunk", "ì•Œ ìˆ˜ ì—†ìŒ")
                st.markdown(f"""
                #### ì²­í¬ {i} (ìœ ì‚¬ë„: {sim:.2f})
                **ì¶œì²˜:** {source} / ì²­í¬ ë²ˆí˜¸: {chunk_num}
                ```text
                {doc.page_content}
                ```
                ---
                """)

        # ğŸ§  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join(doc.page_content for doc, _ in filtered_docs)
        prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {user_query}
ë‹µë³€:
"""

        # ğŸ¤– ëª¨ë¸ ë¡œë”©
        if st.session_state.model is None:
            with st.spinner("ğŸ§  LoRA ëª¨ë¸ ë¡œë”© ì¤‘..."):
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                
                base_model = AutoModelForCausalLM.from_pretrained(
                    "EleutherAI/pythia-70m",
                    device_map="auto",
                    torch_dtype=torch.float16 if st.session_state.device == "cuda" else torch.float32
                )
                tokenizer = AutoTokenizer.from_pretrained("EleutherAI/pythia-70m")
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    base_model.config.pad_token_id = tokenizer.eos_token_id

                model = PeftModel.from_pretrained(base_model, "slm_lora")
                st.session_state.model = model
                st.session_state.tokenizer = tokenizer

        # ğŸ—£ï¸ ì‘ë‹µ ìƒì„±
        inputs = st.session_state.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=True
        ).to(st.session_state.device)
        
        with torch.no_grad():
            outputs = st.session_state.model.generate(
                **inputs,
                max_new_tokens=150,
                num_beams=4,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=st.session_state.tokenizer.pad_token_id
            )
        answer = st.session_state.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ë‹µë³€ í¬ë§·íŒ…
        def format_answer(text):
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            text = re.sub(r'\n\s*\n', '\n\n', text)
            return text

        # ğŸ’¬ ëŒ€í™” ì €ì¥
        st.session_state.chat_history.append(("user", user_query))
        st.session_state.chat_history.append(("assistant", format_answer(answer)))

    except Exception as e:
        st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        st.error(traceback.format_exc())
        st.stop()

# âœ… ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for role, msg in st.session_state.chat_history:
    with st.chat_message(role):
        st.markdown(msg)
