from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch
import os
import re
import gc
from tqdm import tqdm
import multiprocessing

def split_to_chunks(text, min_length=30, max_length=300):
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„ë¦¬
    paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > min_length]
    chunks = []
    
    for para in paragraphs:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œê¸€ ë¬¸ì¥ êµ¬ë¶„ì ì¶”ê°€)
        sentences = re.split(r'(?<=[.!?ã€‚]) +', para)
        current_chunk = ""
        
        for sent in sentences:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´
            potential_length = len(current_chunk) + len(sent) + 1
            
            if potential_length <= max_length:
                current_chunk += (sent + " ").strip()
            else:
                if len(current_chunk.strip()) >= min_length:
                    chunks.append(current_chunk.strip())
                current_chunk = sent + " "
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if len(current_chunk.strip()) >= min_length:
            chunks.append(current_chunk.strip())
    
    return chunks

def create_embeddings():
    print("ğŸ”„ ì„ë² ë”© ìƒì„± ì‹œì‘...")
    
    # CPU ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (ì „ì²´ ì½”ì–´ì˜ 50%ë§Œ ì‚¬ìš©)
    num_threads = max(1, multiprocessing.cpu_count() // 2)
    torch.set_num_threads(num_threads)
    print(f"CPU ìŠ¤ë ˆë“œ ìˆ˜: {num_threads}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cpu"  # CPUë§Œ ì‚¬ìš©
    print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    
    # ë” ê°€ë²¼ìš´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    print("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",  # ë” ê°€ë²¼ìš´ ëª¨ë¸
        model_kwargs={'device': device}
    )
    
    texts = []
    metadatas = []
    
    # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    files = [f for f in os.listdir("chunks") if f.endswith(".txt")]
    print(f"ğŸ“š ì´ {len(files)}ê°œì˜ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for filename in tqdm(files, desc="íŒŒì¼ ì²˜ë¦¬ ì¤‘"):
        try:
            print(f"\nğŸ“„ {filename} ì²˜ë¦¬ ì¤‘...")
            with open(os.path.join("chunks", filename), "r", encoding="utf-8") as f:
                content = f.read()
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• 
                chunks = split_to_chunks(content, min_length=30, max_length=300)
                print(f"  - {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±ë¨")
                
                for idx, chunk in enumerate(chunks):
                    texts.append(chunk)
                    metadatas.append({"source": filename, "chunk": idx})
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if len(texts) % 100 == 0:  # ë” ìì£¼ ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
        except Exception as e:
            print(f"âŒ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue
    
    print(f"\nğŸ“Š ì´ {len(texts)}ê°œì˜ ì²­í¬ ìƒì„±ë¨")
    
    # ì¤‘ë³µ ì œê±° (í…ìŠ¤íŠ¸ ê¸°ì¤€)
    print("ğŸ”„ ì¤‘ë³µ ì œê±° ì¤‘...")
    unique = list({text: meta for text, meta in zip(texts, metadatas)}.items())
    texts = [u[0] for u in unique]
    metadatas = [u[1] for u in unique]
    print(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„ {len(texts)}ê°œì˜ ê³ ìœ í•œ ì²­í¬ ë‚¨ìŒ")
    
    # ë²¡í„° DB ìƒì„±
    print("\nğŸ”„ FAISS ë²¡í„° DB ìƒì„± ì¤‘...")
    try:
        # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬
        batch_size = 100  # ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
        for i in tqdm(range(0, len(texts), batch_size), desc="ë²¡í„° DB ìƒì„± ì¤‘"):
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            
            if i == 0:
                db = FAISS.from_texts(batch_texts, embeddings, metadatas=batch_metadatas)
            else:
                db.add_texts(batch_texts, metadatas=batch_metadatas)
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            gc.collect()
        
        # ì €ì¥
        print("\nğŸ’¾ ë²¡í„° DB ì €ì¥ ì¤‘...")
        db.save_local("embed_faiss")
        print("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë²¡í„° DB ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

if __name__ == "__main__":
    create_embeddings() 